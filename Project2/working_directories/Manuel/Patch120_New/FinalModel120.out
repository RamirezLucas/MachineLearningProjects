Last login: Wed Dec 19 16:58:03 on ttys001
MBP-de-Manuel:~ manuelcordova$ cd Desktop/EPFL/Master/Minor\ CSE/Machine\ Learning/
MBP-de-Manuel:Machine Learning manuelcordova$ cd ML_course/
MBP-de-Manuel:ML_course manuelcordova$ git pull
Already up to date.
MBP-de-Manuel:ML_course manuelcordova$ cd ../MachineLearningProjects/Project2/working_directories/Manuel/
MBP-de-Manuel:Manuel manuelcordova$ cd Patch120_New/
MBP-de-Manuel:Patch120_New manuelcordova$ ls
UNET_patch_120_rot.py	load.py			script_of_script.sh
__pycache__		modelCNN_200.py		submission.py
helper.py		modelUNET_120.py	test_script.sh
img_load.py		patch.py		transformation.py
img_manipulation.py	predict_200.py
MBP-de-Manuel:Patch120_New manuelcordova$ ls
UNET_patch_120_rot.py	load.py			script_of_script.sh
__pycache__		modelCNN_200.py		submission.py
helper.py		modelUNET_120.py	test_script.sh
img_load.py		patch.py		transformation.py
img_manipulation.py	predict_200.py
MBP-de-Manuel:Patch120_New manuelcordova$ vi UNET_patch_120_rot.py 
MBP-de-Manuel:Patch120_New manuelcordova$ python UNET_patch_120_rot.py 
/Users/manuelcordova/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Loading training images...
Done!
Rotating training images for data augmentation...
Done!
Loading validation images...
Done!
Rotating validation images for data augmentation...
Done!
Loading additional images...
Done!
[[[[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  ...

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]]


 [[[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  ...

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]]


 [[[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  ...

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]]


 ...


 [[[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  ...

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [1.]
   [1.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [1.]
   [1.]]

  [[0.]
   [0.]
   [0.]
   ...
   [1.]
   [1.]
   [1.]]]


 [[[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  ...

  [[1.]
   [1.]
   [1.]
   ...
   [1.]
   [1.]
   [1.]]

  [[1.]
   [1.]
   [1.]
   ...
   [1.]
   [1.]
   [1.]]

  [[1.]
   [1.]
   [1.]
   ...
   [1.]
   [1.]
   [1.]]]


 [[[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [1.]
   [1.]
   [1.]]

  ...

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]

  [[0.]
   [0.]
   [0.]
   ...
   [0.]
   [0.]
   [0.]]]]
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
img (InputLayer)                (None, 120, 120, 3)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 120, 120, 16) 448         img[0][0]                        
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 120, 120, 16) 0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 120, 120, 16) 2320        dropout_1[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 60, 60, 16)   0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 60, 60, 32)   4640        max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 60, 60, 32)   0           conv2d_3[0][0]                   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 60, 60, 32)   9248        dropout_2[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 30, 30, 32)   0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 30, 30, 64)   18496       max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 30, 30, 64)   0           conv2d_5[0][0]                   
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 30, 30, 64)   36928       dropout_3[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 64)     0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 5, 5, 128)    73856       max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 5, 5, 128)    0           conv2d_7[0][0]                   
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 5, 5, 128)    147584      dropout_4[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 1, 1, 256)    295168      max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 1, 1, 256)    0           conv2d_9[0][0]                   
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 1, 1, 256)    590080      dropout_5[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 5, 5, 128)    295040      conv2d_10[0][0]                  
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 5, 5, 256)    0           conv2d_transpose_1[0][0]         
                                                                 conv2d_8[0][0]                   
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 5, 5, 128)    295040      concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 5, 5, 128)    0           conv2d_11[0][0]                  
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 5, 5, 128)    147584      dropout_6[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 30, 30, 64)   73792       conv2d_12[0][0]                  
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 30, 30, 128)  0           conv2d_transpose_2[0][0]         
                                                                 conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 30, 30, 64)   73792       concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 30, 30, 64)   0           conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 30, 30, 64)   36928       dropout_7[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 60, 60, 32)   18464       conv2d_14[0][0]                  
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 60, 60, 64)   0           conv2d_transpose_3[0][0]         
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 60, 60, 32)   18464       concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 60, 60, 32)   0           conv2d_15[0][0]                  
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 60, 60, 32)   9248        dropout_8[0][0]                  
__________________________________________________________________________________________________
conv2d_transpose_4 (Conv2DTrans (None, 120, 120, 16) 4624        conv2d_16[0][0]                  
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 120, 120, 32) 0           conv2d_transpose_4[0][0]         
                                                                 conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 120, 120, 16) 4624        concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 120, 120, 16) 0           conv2d_17[0][0]                  
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 120, 120, 16) 2320        dropout_9[0][0]                  
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 120, 120, 1)  17          conv2d_18[0][0]                  
==================================================================================================
Total params: 2,158,705
Trainable params: 2,158,705
Non-trainable params: 0
__________________________________________________________________________________________________
Train on 20118 samples, validate on 320 samples
Epoch 1/80
2018-12-19 18:19:33.411404: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
20118/20118 [==============================] - 1737s 86ms/step - loss: 0.2954 - f1_score: 0.0445 - acc: 0.9077 - val_loss: 0.5072 - val_f1_score: 0.0326 - val_acc: 0.7611

Epoch 00001: val_loss improved from inf to 0.50723, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 2/80
20118/20118 [==============================] - 1747s 87ms/step - loss: 0.1859 - f1_score: 0.1852 - acc: 0.9277 - val_loss: 0.5665 - val_f1_score: 0.1275 - val_acc: 0.7363

Epoch 00002: val_loss did not improve from 0.50723
Epoch 3/80
20118/20118 [==============================] - 1747s 87ms/step - loss: 0.1609 - f1_score: 0.2505 - acc: 0.9374 - val_loss: 0.5797 - val_f1_score: 0.1369 - val_acc: 0.7383

Epoch 00003: val_loss did not improve from 0.50723
Epoch 4/80
20118/20118 [==============================] - 1742s 87ms/step - loss: 0.1483 - f1_score: 0.2823 - acc: 0.9424 - val_loss: 0.4398 - val_f1_score: 0.3340 - val_acc: 0.7646

Epoch 00004: val_loss improved from 0.50723 to 0.43979, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 5/80
20118/20118 [==============================] - 1742s 87ms/step - loss: 0.1380 - f1_score: 0.3053 - acc: 0.9466 - val_loss: 0.3566 - val_f1_score: 0.3626 - val_acc: 0.8137

Epoch 00005: val_loss improved from 0.43979 to 0.35659, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 6/80
20118/20118 [==============================] - 1745s 87ms/step - loss: 0.1302 - f1_score: 0.3221 - acc: 0.9500 - val_loss: 0.3417 - val_f1_score: 0.3834 - val_acc: 0.8294

Epoch 00006: val_loss improved from 0.35659 to 0.34170, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 7/80
20118/20118 [==============================] - 1738s 86ms/step - loss: 0.1252 - f1_score: 0.3332 - acc: 0.9522 - val_loss: 0.3985 - val_f1_score: 0.2936 - val_acc: 0.8008

Epoch 00007: val_loss did not improve from 0.34170
Epoch 8/80
20118/20118 [==============================] - 1746s 87ms/step - loss: 0.1197 - f1_score: 0.3439 - acc: 0.9544 - val_loss: 0.3038 - val_f1_score: 0.4405 - val_acc: 0.8515

Epoch 00008: val_loss improved from 0.34170 to 0.30377, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 9/80
20118/20118 [==============================] - 1738s 86ms/step - loss: 0.1139 - f1_score: 0.3534 - acc: 0.9569 - val_loss: 0.4503 - val_f1_score: 0.2644 - val_acc: 0.8205

Epoch 00009: val_loss did not improve from 0.30377
Epoch 10/80
20118/20118 [==============================] - 1737s 86ms/step - loss: 0.1097 - f1_score: 0.3613 - acc: 0.9587 - val_loss: 0.3119 - val_f1_score: 0.4635 - val_acc: 0.8565

Epoch 00010: val_loss did not improve from 0.30377
Epoch 11/80
20118/20118 [==============================] - 1732s 86ms/step - loss: 0.1073 - f1_score: 0.3661 - acc: 0.9598 - val_loss: 0.2897 - val_f1_score: 0.4415 - val_acc: 0.8666

Epoch 00011: val_loss improved from 0.30377 to 0.28974, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 12/80
20118/20118 [==============================] - 1734s 86ms/step - loss: 0.1034 - f1_score: 0.3725 - acc: 0.9614 - val_loss: 0.2903 - val_f1_score: 0.4671 - val_acc: 0.8676

Epoch 00012: val_loss did not improve from 0.28974
Epoch 13/80
20118/20118 [==============================] - 1733s 86ms/step - loss: 0.1021 - f1_score: 0.3756 - acc: 0.9619 - val_loss: 0.2735 - val_f1_score: 0.4383 - val_acc: 0.8778

Epoch 00013: val_loss improved from 0.28974 to 0.27351, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 14/80
20118/20118 [==============================] - 1730s 86ms/step - loss: 0.0994 - f1_score: 0.3785 - acc: 0.9630 - val_loss: 0.2561 - val_f1_score: 0.3949 - val_acc: 0.8731

Epoch 00014: val_loss improved from 0.27351 to 0.25610, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 15/80
20118/20118 [==============================] - 1730s 86ms/step - loss: 0.0971 - f1_score: 0.3835 - acc: 0.9640 - val_loss: 0.2667 - val_f1_score: 0.3867 - val_acc: 0.8771

Epoch 00015: val_loss did not improve from 0.25610
Epoch 16/80
20118/20118 [==============================] - 1729s 86ms/step - loss: 0.0959 - f1_score: 0.3838 - acc: 0.9644 - val_loss: 0.2547 - val_f1_score: 0.4276 - val_acc: 0.8780

Epoch 00016: val_loss improved from 0.25610 to 0.25474, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 17/80
20118/20118 [==============================] - 1730s 86ms/step - loss: 0.0941 - f1_score: 0.3880 - acc: 0.9651 - val_loss: 0.2619 - val_f1_score: 0.3911 - val_acc: 0.8767

Epoch 00017: val_loss did not improve from 0.25474
Epoch 18/80
20118/20118 [==============================] - 1728s 86ms/step - loss: 0.0914 - f1_score: 0.3916 - acc: 0.9660 - val_loss: 0.2682 - val_f1_score: 0.4245 - val_acc: 0.8850

Epoch 00018: val_loss did not improve from 0.25474
Epoch 19/80
20118/20118 [==============================] - 1727s 86ms/step - loss: 0.0907 - f1_score: 0.3923 - acc: 0.9663 - val_loss: 0.2636 - val_f1_score: 0.4072 - val_acc: 0.8879

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.

Epoch 00019: val_loss did not improve from 0.25474
Epoch 20/80
20118/20118 [==============================] - 1727s 86ms/step - loss: 0.0856 - f1_score: 0.3999 - acc: 0.9681 - val_loss: 0.2451 - val_f1_score: 0.4363 - val_acc: 0.8896

Epoch 00020: val_loss improved from 0.25474 to 0.24508, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 21/80
20118/20118 [==============================] - 1782s 89ms/step - loss: 0.0841 - f1_score: 0.4023 - acc: 0.9686 - val_loss: 0.2442 - val_f1_score: 0.4462 - val_acc: 0.8943

Epoch 00021: val_loss improved from 0.24508 to 0.24423, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 22/80
20118/20118 [==============================] - 1750s 87ms/step - loss: 0.0834 - f1_score: 0.4030 - acc: 0.9689 - val_loss: 0.2406 - val_f1_score: 0.4676 - val_acc: 0.8981

Epoch 00022: val_loss improved from 0.24423 to 0.24059, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 23/80
20118/20118 [==============================] - 1733s 86ms/step - loss: 0.0833 - f1_score: 0.4039 - acc: 0.9689 - val_loss: 0.2506 - val_f1_score: 0.4568 - val_acc: 0.8941

Epoch 00023: val_loss did not improve from 0.24059
Epoch 24/80
20118/20118 [==============================] - 1732s 86ms/step - loss: 0.0826 - f1_score: 0.4041 - acc: 0.9692 - val_loss: 0.2503 - val_f1_score: 0.4574 - val_acc: 0.8947

Epoch 00024: val_loss did not improve from 0.24059
Epoch 25/80
20118/20118 [==============================] - 1727s 86ms/step - loss: 0.0826 - f1_score: 0.4046 - acc: 0.9692 - val_loss: 0.2357 - val_f1_score: 0.4647 - val_acc: 0.8973

Epoch 00025: val_loss improved from 0.24059 to 0.23568, saving model to UNET_patch_120_rot_chkpt.h5
Epoch 26/80
20118/20118 [==============================] - 1726s 86ms/step - loss: 0.0822 - f1_score: 0.4048 - acc: 0.9693 - val_loss: 0.2766 - val_f1_score: 0.4287 - val_acc: 0.8885

Epoch 00026: val_loss did not improve from 0.23568
Epoch 27/80
20118/20118 [==============================] - 1725s 86ms/step - loss: 0.0821 - f1_score: 0.4058 - acc: 0.9694 - val_loss: 0.2461 - val_f1_score: 0.4513 - val_acc: 0.8975

Epoch 00027: val_loss did not improve from 0.23568
Epoch 28/80
20118/20118 [==============================] - 1730s 86ms/step - loss: 0.0818 - f1_score: 0.4058 - acc: 0.9694 - val_loss: 0.2485 - val_f1_score: 0.4574 - val_acc: 0.8976

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.

Epoch 00028: val_loss did not improve from 0.23568
Epoch 29/80
20118/20118 [==============================] - 1727s 86ms/step - loss: 0.0811 - f1_score: 0.4061 - acc: 0.9697 - val_loss: 0.2457 - val_f1_score: 0.4620 - val_acc: 0.8983

Epoch 00029: val_loss did not improve from 0.23568
Epoch 30/80
20118/20118 [==============================] - 1732s 86ms/step - loss: 0.0811 - f1_score: 0.4072 - acc: 0.9696 - val_loss: 0.2498 - val_f1_score: 0.4562 - val_acc: 0.8962

Epoch 00030: val_loss did not improve from 0.23568
Epoch 31/80
20118/20118 [==============================] - 1728s 86ms/step - loss: 0.0809 - f1_score: 0.4073 - acc: 0.9697 - val_loss: 0.2452 - val_f1_score: 0.4621 - val_acc: 0.8985

Epoch 00031: ReduceLROnPlateau reducing learning rate to 1e-05.

Epoch 00031: val_loss did not improve from 0.23568
Epoch 32/80
20118/20118 [==============================] - 1737s 86ms/step - loss: 0.0808 - f1_score: 0.4072 - acc: 0.9698 - val_loss: 0.2446 - val_f1_score: 0.4602 - val_acc: 0.8981

Epoch 00032: val_loss did not improve from 0.23568
Epoch 33/80
20118/20118 [==============================] - 1738s 86ms/step - loss: 0.0809 - f1_score: 0.4071 - acc: 0.9697 - val_loss: 0.2491 - val_f1_score: 0.4573 - val_acc: 0.8967

Epoch 00033: val_loss did not improve from 0.23568
Epoch 34/80
  200/20118 [..............................] - ETA: 27:59 - loss: 0.0778 - f1_score: 0.4167 - acc: 0.9715^CTraceback (most recent call last):
  File "UNET_patch_120_rot.py", line 93, in <module>
    model_train = model.fit(all_imgs, all_gts, batch_size=batch_size,epochs=epochs,callbacks=callbacks,verbose=1,validation_data=(all_val_imgs, all_val_gts))
  File "/Users/manuelcordova/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1039, in fit
    validation_steps=validation_steps)
  File "/Users/manuelcordova/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py", line 199, in fit_loop
    outs = f(ins_batch)
  File "/Users/manuelcordova/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2715, in __call__
    return self._call(inputs)
  File "/Users/manuelcordova/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2675, in _call
    fetched = self._callable_fn(*array_vals)
  File "/Users/manuelcordova/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1439, in __call__
    run_metadata_ptr)
KeyboardInterrupt
MBP-de-Manuel:Patch120_New manuelcordova$ 
